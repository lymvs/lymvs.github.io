<!doctype html><html lang=en><head><title>Resume · Lysandros personal website</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="Lysandros Mavropoulos"><meta name=description content="
  Skills
  
    
    Link to heading
  

Python; SQL; ETL/ELT; Data Modeling; Data Warehousing (Snowflake); Apache Spark
(PySpark, Databricks); Data Orchestration (Dataiku, Prefect); CI/CD & DevOps
(Git, Docker); Cloud: AWS; BI & Analytics Tools (Power BI, Tableau);
Agile/Kanban methodology


  Experience
  
    
    Link to heading
  

Pfizer, Thessaloniki, Greece — Senior Data Engineer
APR 2025 - PRESENT

Developed and optimized end-to-end data pipelines using Python and
Dataiku on AWS, ingesting and transforming large-scale data into
Snowflake data warehouses. Ensured high data quality and timely
availability, which improved analytics query performance and reliability
of insights for global teams.
Co-led the implementation of scalable ETL workflows and automation
scripts, eliminating manual data handling and reducing pipeline runtimes
by an estimated 20%. These efficient workflows freed up analysts’ time
and accelerated delivery of business reports.
Ingested and integrated new data sources into ETL pipelines feeding a
Large Language Model (LLM) based application, enabling accurate user Q&amp;A
by maintaining a clean, query-optimized action data model.

Tech used: Python, Dataiku, Snowflake, AWS (cloud storage and databases), SQL,
Streamlit, Git"><meta name=keywords content><meta name=twitter:card content="summary"><meta name=twitter:title content="Resume"><meta name=twitter:description content="Skills Link to heading Python; SQL; ETL/ELT; Data Modeling; Data Warehousing (Snowflake); Apache Spark (PySpark, Databricks); Data Orchestration (Dataiku, Prefect); CI/CD & DevOps (Git, Docker); Cloud: AWS; BI & Analytics Tools (Power BI, Tableau); Agile/Kanban methodology
Experience Link to heading Pfizer, Thessaloniki, Greece — Senior Data Engineer
APR 2025 - PRESENT
Developed and optimized end-to-end data pipelines using Python and Dataiku on AWS, ingesting and transforming large-scale data into Snowflake data warehouses. Ensured high data quality and timely availability, which improved analytics query performance and reliability of insights for global teams. Co-led the implementation of scalable ETL workflows and automation scripts, eliminating manual data handling and reducing pipeline runtimes by an estimated 20%. These efficient workflows freed up analysts’ time and accelerated delivery of business reports. Ingested and integrated new data sources into ETL pipelines feeding a Large Language Model (LLM) based application, enabling accurate user Q&amp;A by maintaining a clean, query-optimized action data model. Tech used: Python, Dataiku, Snowflake, AWS (cloud storage and databases), SQL, Streamlit, Git"><meta property="og:url" content="https://lysandros.github.io/resume/"><meta property="og:site_name" content="Lysandros personal website"><meta property="og:title" content="Resume"><meta property="og:description" content="Skills Link to heading Python; SQL; ETL/ELT; Data Modeling; Data Warehousing (Snowflake); Apache Spark (PySpark, Databricks); Data Orchestration (Dataiku, Prefect); CI/CD & DevOps (Git, Docker); Cloud: AWS; BI & Analytics Tools (Power BI, Tableau); Agile/Kanban methodology
Experience Link to heading Pfizer, Thessaloniki, Greece — Senior Data Engineer
APR 2025 - PRESENT
Developed and optimized end-to-end data pipelines using Python and Dataiku on AWS, ingesting and transforming large-scale data into Snowflake data warehouses. Ensured high data quality and timely availability, which improved analytics query performance and reliability of insights for global teams. Co-led the implementation of scalable ETL workflows and automation scripts, eliminating manual data handling and reducing pipeline runtimes by an estimated 20%. These efficient workflows freed up analysts’ time and accelerated delivery of business reports. Ingested and integrated new data sources into ETL pipelines feeding a Large Language Model (LLM) based application, enabling accurate user Q&amp;A by maintaining a clean, query-optimized action data model. Tech used: Python, Dataiku, Snowflake, AWS (cloud storage and databases), SQL, Streamlit, Git"><meta property="og:locale" content="en_us"><meta property="og:type" content="article"><link rel=canonical href=https://lysandros.github.io/resume/><link rel=preload href=/fonts/fa-brands-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-regular-400.woff2 as=font type=font/woff2 crossorigin><link rel=preload href=/fonts/fa-solid-900.woff2 as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.c8e4eea149ae1dc7c61ba9b0781793711a4e657f7e07a4413f9abc46d52dffc4.css integrity="sha256-yOTuoUmuHcfGG6mweBeTcRpOZX9+B6RBP5q8RtUt/8Q=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/custom.min.ea80b3408bb98d5677314e5a8a75dccfd6314eb6787d975f94fd3096fa811423.css integrity="sha256-6oCzQIu5jVZ3MU5ainXcz9YxTrZ4fZdflP0wlvqBFCM=" crossorigin=anonymous media=screen><link rel=icon type=image/svg+xml href=/images/favicon.svg sizes=any><link rel=icon type=image/png href=/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/images/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/images/safari-pinned-tab.svg color=#5bbad5></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa-solid fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=https://lysandros.github.io/>Lysandros personal website
</a><input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa-solid fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/>Home</a></li><li class=navigation-item><a class=navigation-link href=/resume/>Resume</a></li><li class=navigation-item><a class=navigation-link href=/projects/>Projects</a></li><li class=navigation-item><a class=navigation-link href=/certifications>Certifications</a></li><li class=navigation-item><a class=navigation-link href=/about/>About</a></li></ul></section></nav><div class=content><section class="container page"><article><header><h1 class=title><a class=title-link href=https://lysandros.github.io/resume/>Resume</a></h1></header><h1 id=skills>Skills
<a class=heading-link href=#skills><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p>Python; SQL; ETL/ELT; Data Modeling; Data Warehousing (Snowflake); Apache Spark
(PySpark, Databricks); Data Orchestration (Dataiku, Prefect); CI/CD & DevOps
(Git, Docker); Cloud: AWS; BI & Analytics Tools (Power BI, Tableau);
Agile/Kanban methodology</p><hr><h1 id=experience>Experience
<a class=heading-link href=#experience><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p><strong>Pfizer, Thessaloniki, Greece</strong> — <em>Senior Data Engineer</em><br>APR 2025 - PRESENT</p><ul><li>Developed and optimized end-to-end data pipelines using Python and
Dataiku on AWS, ingesting and transforming large-scale data into
Snowflake data warehouses. Ensured high data quality and timely
availability, which improved analytics query performance and reliability
of insights for global teams.</li><li>Co-led the implementation of scalable ETL workflows and automation
scripts, eliminating manual data handling and reducing pipeline runtimes
by an estimated 20%. These efficient workflows freed up analysts’ time
and accelerated delivery of business reports.</li><li>Ingested and integrated new data sources into ETL pipelines feeding a
Large Language Model (LLM) based application, enabling accurate user Q&amp;A
by maintaining a clean, query-optimized action data model.</li></ul><p>Tech used: Python, Dataiku, Snowflake, AWS (cloud storage and databases), SQL,
Streamlit, Git</p><p><strong>Pfizer, Thessaloniki, Greece</strong> — <em>Data Analyst | Central Monitor</em><br>SEP 2023 - MAR 2025</p><ul><li>Automated recurring clinical data monitoring tasks by developing Python
scripts and Streamlit apps, cutting manual effort for the Central
Monitoring team by ~30% and allowing team members to focus on critical
risk analysis.</li><li>Analyzed clinical trial data (using Python and SAS) to identify abnormal
site performance and data anomalies. Delivered timely alerts and reports
that enabled study teams to intervene at underperforming sites,
contributing to improved data integrity and patient safety.</li><li>Prepared and presented data reports and visualizations for internal
stakeholders. Ensured that insights on trial progress and site outliers
were clearly communicated, supporting compliance with regulatory requirements
and informed decision-making.<br>(Previous role at Pfizer involved strict adherence to data privacy and
GCP/GDPR standards while handling sensitive health data.)</li></ul><p>Tech used: Python, SAS, CluePoints (central monitoring platform), Databricks
(PySpark), Streamlit, Git</p><p><strong>Baresquare, Thessaloniki, Greece</strong> — <em>Data Analyst</em><br>JUL 2022 - APR 2023</p><ul><li>Generated regular performance reports (daily, monthly, quarterly) for a
major e-commerce client using Adobe Analytics and Domo. Ensured on-time
delivery of insights on website traffic and sales, helping the client
track KPIs and marketing campaign effectiveness.</li><li>Monitored an AI-powered anomaly detection system for digital analytics
data, investigating alert tickets each day. Pinpointed root causes of
anomalies (e.g., tagging issues, traffic spikes, bot detection) and
communicated findings to the client team, improving data trust and
enabling prompt fixes to data issues.</li></ul><p>Tech used: Adobe Analytics, Domo (BI), Excel, PowerBI</p><p><strong>Masoutes S.A., Thessaloniki, Greece</strong> — <em>Logistics Data Analyst</em><br>SEP 2017 - JUL 2022</p><ul><li>Analyzed supply chain and warehouse data to find trends and inefficiencies
in inventory distribution and delivery routes. Provided
actionable insights to the logistics management team that informed
decisions on optimizing stock levels and transportation routes.</li><li>Automated reporting and data extraction processes using Python and SQL,
replacing several manual Excel-based workflows. This automation saved
approximately 10-15 hours of work per week and significantly reduced
reporting errors, leading to more reliable operations monitoring.</li></ul><p>Tech used: Python, SQL, Mantis WMS (warehouse management system), Excel</p><hr><h1 id=projects>Projects
<a class=heading-link href=#projects><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p><strong>Vehicle Routing Problem Solver</strong> (Personal Project)</p><p>Developed a web application to solve complex Vehicle Routing Problem variants
for logistics optimization. Implemented using Python (Flask) with Google
OR-Tools for route optimization, NetworkX for graph operations, and Geopy for
geocoding. The tool provides the resulting routes, helping users minimize total
route distance and cost in delivery scenarios.</p><p>GitHub: <a href=https://github.com/lymvs/vrp.git class=external-link target=_blank rel=noopener>https://github.com/lymvs/vrp.git</a><br>Demo Video: <a href=https://youtu.be/waHRrTyVWCs class=external-link target=_blank rel=noopener>https://youtu.be/waHRrTyVWCs</a></p><p><strong>Data Loader</strong> (Work Project)</p><p>Created an automated ingestion pipeline to load CSV files into Databricks with a
medallion architecture approach. The tool ingests raw files into a Bronze table
(for archival and audit trail) and then transforms and deduplicates data into a
Silver table (clean, analysis-ready). This project improved data ingestion
efficiency for the team, ensuring data integrity.</p><p>Tech: Python, Streamlit, Posit Connect (deployment platform), Git, Databricks</p><p><strong>Central Monitoring Toolkit</strong> (Work Project)</p><p>Built a Streamlit web application to automate frequent tasks in Pfizer’s Central
Monitoring operations. This tool integrated with file systems (SMB network
drives) and internal databases to pull clinical trial metrics and generate
on-demand reports, reducing manual effort and minimizing errors in critical
monitoring workflows.</p><p>Tech: Python, Streamlit, Posit Connect (deployment platform), Git, Databricks</p><hr><h1 id=education>Education
<a class=heading-link href=#education><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p><strong>University of Macedonia, Greece</strong> — <em>Master in Business Analytics & Data Science</em><br>FEB 2020 - JAN 2022<br>Grade: 8.33/10. Awarded Performance Scholarship for academic excellence.<br>Thesis: “The Vehicle Routing Problem of Masoutis S.A.”. (Grade: 9/10)</p><p><strong>Technological Educational Institute of Central Macedonia, Greece</strong>
— <em>Master in Logistics & Supply Chains</em><br>OCT 2015 - JAN 2022<br>Grade: 8.83/10.<br>Thesis: “Vehicle Routing Optimization in a Retail Company”. (Grade: 10/10)</p><hr><h1 id=certifications>Certifications
<a class=heading-link href=#certifications><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p><strong>Dataiku Academy</strong> - <em>Dataiku Advanced Designer</em><br>APR 2025<br>Certificate: <a href=https://verify.skilljar.com/c/3k2i4g5a7mq5 class=external-link target=_blank rel=noopener>https://verify.skilljar.com/c/3k2i4g5a7mq5</a></p><p><strong>Dataiku Academy</strong> - <em>Dataiku Core Designer</em><br>APR 2025<br>Certificate: <a href=https://verify.skilljar.com/c/drdwgautevx9 class=external-link target=_blank rel=noopener>https://verify.skilljar.com/c/drdwgautevx9</a></p><p><strong>Harvard University</strong> - <em>CS50’s Introduction to Computer Science</em><br>JUL 2024 - DEC 2024<br>Certificate:<br><a href=https://certificates.cs50.io/42792564-6ebb-4ba8-bcb5-606cfc17b376.png class=external-link target=_blank rel=noopener>https://certificates.cs50.io/42792564-6ebb-4ba8-bcb5-606cfc17b376.png</a><br>(Final project involved developing the above-mentioned VRP solver tool.)</p><hr><h1 id=achievements>Achievements
<a class=heading-link href=#achievements><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p><strong>Performance Scholarship (2022)</strong><br>Awarded a performance scholarship after the completion of the second semester of
the MSc program in Business Analytics and Data Science.</p><p><strong>Research Presentation</strong><br>Authored and presented a paper on Vehicle Routing Optimization in Retail at the
4th International Conference on Supply Chains (Katerini, 2018).</p><hr><h1 id=additional-information>Additional Information
<a class=heading-link href=#additional-information><i class="fa-solid fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><p><strong>Work Authorization</strong><br>EU citizen - eligible to work in EU countries <strong>without</strong> visa sponsorship (valid
work rights across the EU/EEA).</p><p><strong>Relocation</strong><br>Open to relocation within Europe.</p></article></section></div><footer class=footer><section class=container>©
2025
Lysandros Mavropoulos
·
Powered by <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/ target=_blank rel=noopener>Coder</a>.</section></footer></main><script src=/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script></body></html>